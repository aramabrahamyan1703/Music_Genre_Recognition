{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /Users/aram/anaconda3/lib/python3.11/site-packages (1.7.4.2)\n",
      "Requirement already satisfied: bleach in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (3.7)\n",
      "Requirement already satisfied: protobuf in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (5.27.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: requests in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (68.0.0)\n",
      "Requirement already satisfied: six>=1.10 in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (2.3.0)\n",
      "Requirement already satisfied: webencodings in /Users/aram/anaconda3/lib/python3.11/site-packages (from kaggle) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification\n",
      "License(s): other\n",
      "^C\n",
      "User cancelled operation\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download -d andradaolteanu/gtzan-dataset-music-genre-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"gtzan-dataset-music-genre-classification.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"GTZAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('GTZAN/Data/features_30_sec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt((np.sum(x1-x2))**2)\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=11):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict_all(self, X_test):\n",
    "        return [self.predict(x) for x in X_test]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        distance = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        k_index = np.argsort(distance)[:self.k]\n",
    "        k_labels = [self.y_train[i] for i in k_index]\n",
    "        winner = Counter(list(k_labels)).most_common(1)\n",
    "        return winner[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df.iloc[:, 1:-1].values  # Feature columns\n",
    "y = df[\"label\"].values  # Genre labels\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = knn.predict_all(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       blues       0.15      0.10      0.12        20\n",
      "   classical       0.40      0.62      0.48        13\n",
      "     country       0.18      0.19      0.18        27\n",
      "       disco       0.17      0.19      0.18        21\n",
      "      hiphop       0.05      0.07      0.06        15\n",
      "        jazz       0.27      0.18      0.22        22\n",
      "       metal       0.35      0.24      0.29        25\n",
      "         pop       0.16      0.38      0.22        13\n",
      "      reggae       0.31      0.17      0.22        23\n",
      "        rock       0.06      0.05      0.05        21\n",
      "\n",
      "    accuracy                           0.20       200\n",
      "   macro avg       0.21      0.22      0.20       200\n",
      "weighted avg       0.21      0.20      0.20       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class Naive_Bayes:\n",
    "    def __init__(self, distribution='Gaussian', epsilon=1e-6, laplace=1):\n",
    "        \"\"\"\n",
    "        distribution: 'Gaussian' for continuous features, 'multinomial' for discrete features.\n",
    "        epsilon: small constant added to standard deviation (for Gaussian) to avoid division by zero.\n",
    "        laplace: Laplace smoothing constant for discrete features.\n",
    "        \"\"\"\n",
    "        self.distribution = distribution.lower()\n",
    "        self.epsilon = epsilon\n",
    "        self.laplace = laplace\n",
    "        self.class_priors = {}\n",
    "        self.feature_likelihoods = {}\n",
    "        self.label_encoder = None\n",
    "\n",
    "    def encode_labels(self, y):\n",
    "        \"\"\"Encode string labels to integers.\"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        return self.label_encoder.fit_transform(y)\n",
    "    \n",
    "    def inverse_transform_labels(self, encoded_labels):\n",
    "        \"\"\"\n",
    "        Convert encoded labels back to the original labels.\n",
    "        \n",
    "        Parameters:\n",
    "            encoded_labels (array-like): Encoded labels from prediction.\n",
    "            \n",
    "        Returns:\n",
    "            array-like: Original labels.\n",
    "        \"\"\"\n",
    "        if self.label_encoder is None:\n",
    "            raise ValueError(\"Label encoder not found. Ensure you encoded labels during fit.\")\n",
    "        return self.label_encoder.inverse_transform(encoded_labels)\n",
    "\n",
    "    def _fit_discrete(self, x, y):\n",
    "        \"\"\"\n",
    "        Fit model for discrete features using Laplace smoothing.\n",
    "        For each class and feature, compute:\n",
    "            P(feature_value | class) = (count + laplace) / (N_class + laplace * k)\n",
    "        where k is the number of unique values for that feature.\n",
    "        \"\"\"\n",
    "        self.classes, class_counts = np.unique(y, return_counts=True)\n",
    "        total_count = len(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # Compute class priors with smoothing\n",
    "        for cls, count in zip(self.classes, class_counts):\n",
    "            self.class_priors[cls] = (count + self.laplace) / (total_count + n_classes * self.laplace)\n",
    "        \n",
    "        self.feature_likelihoods = {}\n",
    "        n_features = x.shape[1]\n",
    "        for cls in self.classes:\n",
    "            X_cls = x[y == cls]\n",
    "            likelihoods = []\n",
    "            for j in range(n_features):\n",
    "                # Get all possible values for feature j (from the entire dataset)\n",
    "                unique_vals = np.unique(x[:, j])\n",
    "                k = len(unique_vals)\n",
    "                col = X_cls[:, j]\n",
    "                values, counts = np.unique(col, return_counts=True)\n",
    "                counts_dict = {val: cnt for val, cnt in zip(values, counts)}\n",
    "                # Denominator used in smoothing:\n",
    "                denom = len(col) + self.laplace * k\n",
    "                likelihood = {}\n",
    "                for v in unique_vals:\n",
    "                    count_v = counts_dict.get(v, 0)\n",
    "                    likelihood[v] = (count_v + self.laplace) / denom\n",
    "                # Store a default probability for unseen values:\n",
    "                default_prob = self.laplace / denom\n",
    "                # Save as a tuple: (likelihood dictionary, default probability)\n",
    "                likelihoods.append((likelihood, default_prob))\n",
    "            self.feature_likelihoods[cls] = likelihoods\n",
    "    \n",
    "    def _predict_probabilities_discrete(self, x):\n",
    "        \"\"\"Predict probabilities for discrete features using the smoothed likelihoods.\"\"\"\n",
    "        n_samples = x.shape[0]\n",
    "        probabilities = []\n",
    "        n_features = x.shape[1]\n",
    "        for cls in self.classes:\n",
    "            prob = np.full(n_samples, self.class_priors[cls])\n",
    "            for j in range(n_features):\n",
    "                likelihood_dict, default_prob = self.feature_likelihoods[cls][j]\n",
    "                col = x[:, j]\n",
    "                # For each sample, multiply by the likelihood for the observed feature value.\n",
    "                feature_probs = np.array([likelihood_dict.get(val, default_prob) for val in col])\n",
    "                prob *= feature_probs\n",
    "            probabilities.append(prob)\n",
    "        return np.array(probabilities)\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Public Fit and Predict Methods\n",
    "    # ------------------------------\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        Fit the Naive Bayes model.\n",
    "        If using continuous features (Gaussian), compute mean and std for each feature per class.\n",
    "        If using discrete features (multinomial), compute smoothed likelihoods.\n",
    "        \"\"\"\n",
    "        if self.distribution == 'gaussian':\n",
    "            self._fit_continuous(x, y)\n",
    "        elif self.distribution == 'multinomial':\n",
    "            self._fit_discrete(x, y)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distribution. Use 'Gaussian' or 'multinomial'.\")\n",
    "    \n",
    "    def predict_probabilities(self, x):\n",
    "        \"\"\"\n",
    "        Return an array of predicted probabilities of shape (n_classes, n_samples).\n",
    "        \"\"\"\n",
    "        if self.distribution == 'gaussian':\n",
    "            return self._predict_probabilities_continuous(x)\n",
    "        elif self.distribution == 'multinomial':\n",
    "            return self._predict_probabilities_discrete(x)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distribution.\")\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class for each sample.\n",
    "        Returns an array of predicted class indices.\n",
    "        \"\"\"\n",
    "        probabilities = self.predict_probabilities(x)\n",
    "        return np.argmax(probabilities, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:-1].values  # Feature columns\n",
    "y = df[\"label\"].values  # Genre labels\n",
    "\n",
    "nb = Naive_Bayes()\n",
    "\n",
    "y = nb.encode_labels(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = Naive_Bayes(distribution='multinomial', laplace=1)\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 7, 3, 2, 2, 7, 3, 5, 8, 5, 9, 9, 8, 5, 2, 6, 5, 3, 7, 9, 4, 0,\n",
       "       6, 2, 5, 5, 2, 9, 5, 5, 3, 0, 7, 7, 7, 5, 7, 9, 5, 5, 9, 2, 2, 2,\n",
       "       2, 5, 5, 8, 4, 5, 9, 8, 2, 5, 5, 5, 7, 0, 5, 9, 9, 2, 5, 4, 6, 6,\n",
       "       6, 6, 2, 5, 5, 9, 2, 7, 0, 2, 0, 9, 2, 9, 3, 0, 5, 0, 9, 9, 5, 6,\n",
       "       6, 2, 3, 9, 9, 2, 7, 4, 3, 9, 4, 2, 7, 7, 3, 5, 6, 6, 6, 9, 2, 9,\n",
       "       5, 9, 5, 2, 9, 4, 2, 5, 5, 2, 8, 5, 8, 5, 5, 5, 5, 4, 7, 9, 2, 7,\n",
       "       9, 9, 2, 2, 2, 5, 4, 9, 2, 5, 7, 2, 8, 9, 9, 0, 5, 5, 6, 2, 5, 3,\n",
       "       9, 4, 9, 2, 5, 6, 9, 7, 5, 2, 8, 6, 9, 8, 2, 5, 3, 7, 9, 5, 5, 3,\n",
       "       9, 2, 9, 8, 5, 6, 6, 5, 6, 0, 3, 3, 0, 6, 9, 5, 5, 5, 9, 9, 2, 2,\n",
       "       6, 9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.15      0.20        20\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.23      0.30      0.26        27\n",
      "           3       0.46      0.29      0.35        21\n",
      "           4       0.56      0.33      0.42        15\n",
      "           5       0.19      0.41      0.26        22\n",
      "           6       0.47      0.36      0.41        25\n",
      "           7       0.41      0.54      0.47        13\n",
      "           8       0.50      0.22      0.30        23\n",
      "           9       0.18      0.33      0.23        21\n",
      "\n",
      "    accuracy                           0.29       200\n",
      "   macro avg       0.33      0.29      0.29       200\n",
      "weighted avg       0.33      0.29      0.29       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes:\n",
    "    def prior_prob(self, X, y):\n",
    "        self.prior_p = (X.groupby(y).apply(lambda x: len(x) / self.rows)).to_numpy()\n",
    "        return self.prior_p\n",
    "    \n",
    "    def post_prob(self, X, y):\n",
    "        self.cond_p = X.groupby(y).apply(lambda x: x.sum() / len(x)).to_numpy()\n",
    "        return self.cond_p\n",
    "    \n",
    "    def postProb(self, x):\n",
    "\n",
    "        posteriors = []\n",
    "\n",
    "        for i in range(self.count):\n",
    "\n",
    "            prior = np.log(self.prior[i]) \n",
    "            conditional = np.sum(np.log(self.densGauss(i, x))) \n",
    "            posterior = prior + conditional\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.count = len(self.classes)\n",
    "        self.num_feature = X.shape[1]\n",
    "        self.rows = X.shape[0]\n",
    "        \n",
    "        self.statParamteres(X, y)\n",
    "        self.priorProb(X, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        preds = [self.postProb(f) for f in X.to_numpy()]\n",
    "        return preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
